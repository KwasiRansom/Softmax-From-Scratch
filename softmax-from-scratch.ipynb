{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021686,
     "end_time": "2021-12-06T00:12:09.072101",
     "exception": false,
     "start_time": "2021-12-06T00:12:09.050415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019964,
     "end_time": "2021-12-06T00:12:09.112096",
     "exception": false,
     "start_time": "2021-12-06T00:12:09.092132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "   The goal of this notebook is is to explain and implement Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn).\n",
    "\n",
    "   Logistic regression is using a probability between 0 and 1 (based on a sigmoid function) to identify which of two categories a probability falls into (eg. this is the number 5 or is not the number 5). \n",
    "   \n",
    "   Softmax regression is the same thing as logistic regression, but it uses more than two possible outcomes (or output classes), outputs a probability between 0 and 1 for each outcome, and then based on which of those outcomes has the highest probability (eg. \"I am 80% sure that I am looking at 4. I am going to say it is a 4.\") . This entails that the classes that we are classifying be exclusive (eg. the single digit number we are identifying is not both an 8 and a 0, it is one or the other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019325,
     "end_time": "2021-12-06T00:12:09.151135",
     "exception": false,
     "start_time": "2021-12-06T00:12:09.131810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to work on the MNIST Data set as the classes are exclusive (they are digits 0-9 and they are only one digit at a time, not 5 and 3 both for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:09.196161Z",
     "iopub.status.busy": "2021-12-06T00:12:09.195519Z",
     "iopub.status.idle": "2021-12-06T00:12:58.878613Z",
     "shell.execute_reply": "2021-12-06T00:12:58.879089Z",
     "shell.execute_reply.started": "2021-12-05T23:28:02.590413Z"
    },
    "papermill": {
     "duration": 49.708745,
     "end_time": "2021-12-06T00:12:58.879253",
     "exception": false,
     "start_time": "2021-12-06T00:12:09.170508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:58.923736Z",
     "iopub.status.busy": "2021-12-06T00:12:58.923179Z",
     "iopub.status.idle": "2021-12-06T00:12:58.929122Z",
     "shell.execute_reply": "2021-12-06T00:12:58.928615Z",
     "shell.execute_reply.started": "2021-12-05T23:28:34.645203Z"
    },
    "papermill": {
     "duration": 0.029858,
     "end_time": "2021-12-06T00:12:58.929215",
     "exception": false,
     "start_time": "2021-12-06T00:12:58.899357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = mnist['data'], mnist['target']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034471,
     "end_time": "2021-12-06T00:12:58.986368",
     "exception": false,
     "start_time": "2021-12-06T00:12:58.951897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see in the shape of our data, X, there are 70,000 example handwritten digits, and each one is made up of 784 pixels (a 28 x 28 box). \n",
    "\n",
    "The value for each pixel, as seen below for the first example, can range from 0 (white) to 255 (black) in grayscale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:59.079414Z",
     "iopub.status.busy": "2021-12-06T00:12:59.078506Z",
     "iopub.status.idle": "2021-12-06T00:12:59.100983Z",
     "shell.execute_reply": "2021-12-06T00:12:59.101730Z",
     "shell.execute_reply.started": "2021-12-05T23:28:34.652917Z"
    },
    "papermill": {
     "duration": 0.075019,
     "end_time": "2021-12-06T00:12:59.101939",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.026920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   3.,  18.,  18.,  18., 126., 136., 175.,  26., 166., 255.,\n",
       "        247., 127.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94.,\n",
       "        154., 170., 253., 253., 253., 253., 253., 225., 172., 253., 242.,\n",
       "        195.,  64.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253.,\n",
       "        253., 253., 253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,\n",
       "         39.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 219., 253., 253.,\n",
       "        253., 253., 253., 198., 182., 247., 241.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107.,\n",
       "        253., 253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14.,   1.,\n",
       "        154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         11., 190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  81., 240., 253., 253., 119.,  25.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0., 249., 253., 249.,  64.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  39., 148., 229., 253., 253., 253., 250., 182.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24.,\n",
       "        114., 221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213.,\n",
       "        253., 253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "        253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  55., 172., 226., 253., 253., 253., 253.,\n",
       "        244., 133.,  11.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,\n",
       "         16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].reshape(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035005,
     "end_time": "2021-12-06T00:12:59.169484",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.134479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will split up the 70,000 example hand-written digits below into an 80%/20% split between training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:59.244518Z",
     "iopub.status.busy": "2021-12-06T00:12:59.243330Z",
     "iopub.status.idle": "2021-12-06T00:12:59.412869Z",
     "shell.execute_reply": "2021-12-06T00:12:59.413726Z",
     "shell.execute_reply.started": "2021-12-05T23:28:34.674248Z"
    },
    "papermill": {
     "duration": 0.210443,
     "end_time": "2021-12-06T00:12:59.413929",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.203486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:59.490461Z",
     "iopub.status.busy": "2021-12-06T00:12:59.488630Z",
     "iopub.status.idle": "2021-12-06T00:12:59.493312Z",
     "shell.execute_reply": "2021-12-06T00:12:59.492633Z",
     "shell.execute_reply.started": "2021-12-05T23:28:34.853072Z"
    },
    "papermill": {
     "duration": 0.045233,
     "end_time": "2021-12-06T00:12:59.493461",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.448228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03447,
     "end_time": "2021-12-06T00:12:59.561954",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.527484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As the labels come in the form of strings, here I am converting them to numbers based on their index in a list of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:59.631128Z",
     "iopub.status.busy": "2021-12-06T00:12:59.630412Z",
     "iopub.status.idle": "2021-12-06T00:12:59.633500Z",
     "shell.execute_reply": "2021-12-06T00:12:59.633071Z",
     "shell.execute_reply.started": "2021-12-05T23:54:35.034518Z"
    },
    "papermill": {
     "duration": 0.036204,
     "end_time": "2021-12-06T00:12:59.633595",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.597391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorical_to_num(y):\n",
    "    nums = np.zeros(y.size)\n",
    "    for i in range(len(np.unique(y))):\n",
    "        nums[np.where(y==np.unique(y)[i])] = i\n",
    "    return nums.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057817,
     "end_time": "2021-12-06T00:12:59.713141",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.655324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we convert the labels in their number form to a one-hot encoding (puts a one in the column of a row the same length as the number of possible options). This will become needed later when calculating loss and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:59.761782Z",
     "iopub.status.busy": "2021-12-06T00:12:59.760882Z",
     "iopub.status.idle": "2021-12-06T00:12:59.762499Z",
     "shell.execute_reply": "2021-12-06T00:12:59.763005Z",
     "shell.execute_reply.started": "2021-12-05T23:54:35.320451Z"
    },
    "papermill": {
     "duration": 0.028938,
     "end_time": "2021-12-06T00:12:59.763112",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.734174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    shape = (y.size, int(np.max(y) + 1))\n",
    "    rows = np.arange(y.size)\n",
    "    one_hot = np.zeros(shape)\n",
    "    one_hot[rows, y] = 1.\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020715,
     "end_time": "2021-12-06T00:12:59.804900",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.784185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To keep the calculations from becoming too large and not being able to compute, I divide all X values by 255 to start, as 255 is the maximum possible value. This is called normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:12:59.852956Z",
     "iopub.status.busy": "2021-12-06T00:12:59.852314Z",
     "iopub.status.idle": "2021-12-06T00:13:00.649069Z",
     "shell.execute_reply": "2021-12-06T00:13:00.648406Z",
     "shell.execute_reply.started": "2021-12-05T23:54:35.630851Z"
    },
    "papermill": {
     "duration": 0.823338,
     "end_time": "2021-12-06T00:13:00.649220",
     "exception": false,
     "start_time": "2021-12-06T00:12:59.825882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (56000, 785) \n",
      " X_val size: (14000, 785) \n",
      " y_train size: (56000,) \n",
      " y_val size: (14000,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#normalize X before we start\n",
    "X_with_bias = X_with_bias/255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_with_bias, y, train_size = 0.8, random_state = 42)\n",
    "print(\"X_train size:\", X_train.shape, '\\n',\n",
    "    \"X_val size:\", X_val.shape, '\\n',\n",
    "    \"y_train size:\", y_train.shape, '\\n',\n",
    "    \"y_val size:\", y_val.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:00.732258Z",
     "iopub.status.busy": "2021-12-06T00:13:00.720380Z",
     "iopub.status.idle": "2021-12-06T00:13:00.829932Z",
     "shell.execute_reply": "2021-12-06T00:13:00.829344Z",
     "shell.execute_reply.started": "2021-12-05T23:54:36.401936Z"
    },
    "papermill": {
     "duration": 0.157733,
     "end_time": "2021-12-06T00:13:00.830086",
     "exception": false,
     "start_time": "2021-12-06T00:13:00.672353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting y validation set to one hot\n",
    "nums = categorical_to_num(y_val)\n",
    "y_val_one_hot = to_one_hot(nums)\n",
    "y_val_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:00.879455Z",
     "iopub.status.busy": "2021-12-06T00:13:00.878517Z",
     "iopub.status.idle": "2021-12-06T00:13:01.449806Z",
     "shell.execute_reply": "2021-12-06T00:13:01.450253Z",
     "shell.execute_reply.started": "2021-12-05T23:54:36.540162Z"
    },
    "papermill": {
     "duration": 0.597657,
     "end_time": "2021-12-06T00:13:01.450375",
     "exception": false,
     "start_time": "2021-12-06T00:13:00.852718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting y training set to one hot\n",
    "nums = categorical_to_num(y_train)\n",
    "y_train_one_hot = to_one_hot(nums)\n",
    "y_train_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022135,
     "end_time": "2021-12-06T00:13:01.494933",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.472798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here are the equations that we will need for building out the softmax regressor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0216,
     "end_time": "2021-12-06T00:13:01.538565",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.516965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Softmax Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021821,
     "end_time": "2021-12-06T00:13:01.582405",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.560584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This equation is used for scoring probabilities. That means combining our prior or initial thought of the weight or importance that each pixel of a handwritten digit make it part of a certain class, with a new handwritten digit (\"based on what I think up front how much do I think that this new digit is a 4?\").\n",
    "\n",
    "$s_k$ is the score (logits) for class $k$. \n",
    "\n",
    "$\\mathbf{x}$ is the batch of handwritten digits you are supplying the model.\n",
    "\n",
    "$\\mathbf{\\theta}$ (theta) is the weight for the class ${k}$ that we are working with.\n",
    "\n",
    "The $^T$ next to $\\mathbf{\\theta}$ shows that we are going to transpose it before taking the dot product of it and our handwritten digits $\\mathbf{x}$.\n",
    "\n",
    "\n",
    "\n",
    "$$s_k(\\mathbf{x}) = (\\mathbf{\\theta}^{(k)})^T\\mathbf{x}$$\n",
    "\n",
    "As this is the dot product of $\\mathbf{\\theta}$ and $\\mathbf{x}$, we have to make sure that our dimensions match (the last dimension of the first matrix is the same as the first dimension of the second matrix). $\\mathbf{x}$ will be in the shape of (m, n_inputs) and $\\mathbf{\\theta}$ will be in the shape of (n_inputs, n_outputs).\n",
    "\n",
    "Notice how we implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:01.632425Z",
     "iopub.status.busy": "2021-12-06T00:13:01.631427Z",
     "iopub.status.idle": "2021-12-06T00:13:01.633422Z",
     "shell.execute_reply": "2021-12-06T00:13:01.633885Z",
     "shell.execute_reply.started": "2021-12-05T23:54:37.275816Z"
    },
    "papermill": {
     "duration": 0.029524,
     "end_time": "2021-12-06T00:13:01.634005",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.604481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SoftmaxScore(x, theta):\n",
    "    score = x@theta\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021883,
     "end_time": "2021-12-06T00:13:01.678402",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.656519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022606,
     "end_time": "2021-12-06T00:13:01.723278",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.700672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The $\\mathbf{\\sigma}$ function puts the scores we just got and puts them on a limited range of probabilities (between 0 and 1). This results in an s shaped graph gets infinitely close to 0 on the left side, and infinitely close to 1 on the right side (being able to visualize these probabilities helps with intuition).\n",
    "\n",
    "$\\hat{p}$ Shows that we are aiming to define probabilities.\n",
    "\n",
    "The $i$ in $\\mathbf{\\sigma} {(\\overrightarrow z)}_i$ lets you know that we are doing this sigmoid function for just one example (instance) of a handwritten digit.\n",
    "\n",
    "$\\overrightarrow z$ are the logits (the dot product of the weights $\\mathbf{\\theta}$ and instances $\\mathbf{x}$).\n",
    "\n",
    "$e^{zi}$ Is Euler's constant to the power of the score $z$ we just found for an individual class $i$ (one possible digit the instance could be). $e^{zj}$ Is the same idea, but we are taking the sum of all the possible digits it could be (0-9) so we use a different character $j$ to represent it.\n",
    "\n",
    "This is called taking the exponent of a number. In this case that number is the logit we computed in the softmax score function.\n",
    "\n",
    "The reason we use Euler's constant here is because it allows us to focus on the exponent's value itself (because in exponential equations the derivative of Euler's constant to any power is itself).\n",
    "\n",
    "$\\sum_{j=1}^{K}$ is the sum of the exponent of each logit ($e$ to the power of each logit over all classes for all handwritten digits.\n",
    "\n",
    "This is referred to as normalizing the exponent of the score.\n",
    "\n",
    "$$\\hat{p} = \\mathbf{\\sigma} {(\\overrightarrow z)}_i = \\frac{e^{zi}}{{\\sum_{j=1}^{K} e^{zj}}}$$\n",
    "\n",
    "Here's how we will implement it (broken up so that it's not as computationally expensive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:01.776535Z",
     "iopub.status.busy": "2021-12-06T00:13:01.775726Z",
     "iopub.status.idle": "2021-12-06T00:13:01.778650Z",
     "shell.execute_reply": "2021-12-06T00:13:01.778245Z"
    },
    "papermill": {
     "duration": 0.033149,
     "end_time": "2021-12-06T00:13:01.778741",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.745592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SoftmaxFunction(score):\n",
    "    exps = np.exp(score)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022896,
     "end_time": "2021-12-06T00:13:01.824776",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.801880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Softmax Regression Classifier Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022971,
     "end_time": "2021-12-06T00:13:01.871023",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.848052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This function states that the class with the highest probability (the digit that it is most likely to be) will be chosen as the prediction for a given handwritten digit.\n",
    "\n",
    "$\\hat{y}$ (or y hat) represents our final prediction of what handwritten an instance is.\n",
    "\n",
    "$$\\hat{y} = \\underset{k}{\\operatorname{argmax}} \\mathbf{\\sigma}(\\mathbf{s}(\\mathbf{x}))_k = \\underset{k}{\\operatorname{argmax}}s_k(\\mathbf{x}) = \\underset{k}{\\operatorname{argmax}} ((\\mathbf{\\theta}^{(k)})^T\\mathbf{x}) $$\n",
    "\n",
    "We will implement it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:01.922180Z",
     "iopub.status.busy": "2021-12-06T00:13:01.921597Z",
     "iopub.status.idle": "2021-12-06T00:13:01.925833Z",
     "shell.execute_reply": "2021-12-06T00:13:01.925386Z",
     "shell.execute_reply.started": "2021-12-05T23:54:39.178445Z"
    },
    "papermill": {
     "duration": 0.031453,
     "end_time": "2021-12-06T00:13:01.925930",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.894477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Prediction(p_hat):\n",
    "    y_hat = np.argmax(p_hat, axis = 1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022405,
     "end_time": "2021-12-06T00:13:01.970385",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.947980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross Entropy Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022674,
     "end_time": "2021-12-06T00:13:02.015202",
     "exception": false,
     "start_time": "2021-12-06T00:13:01.992528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This cost function identifies how wrong a prediction was using the natural log of its probability, and multiplies by 1 (if that class was the correct one), then all of these distances get averaged together for a collective distance from the correct predictions.\n",
    "\n",
    "${J}(\\mathbf{\\Theta})$ represents the cost over the weight vector of the entire batch\n",
    "\n",
    "$m$ is the number of instances in the batch\n",
    "\n",
    "$i$ is one such instance\n",
    "\n",
    "$k$ is one possible class\n",
    "\n",
    "$K$ is a set of all of the classes\n",
    "\n",
    "$log$ is the natural log (again the base is $e$ because it simplifies our math, because the slope of the graph is constant)\n",
    "\n",
    "$y_k^{(i)}$ is either a 0 (if the handwritten digit was not of that class) or 1 (if it was part of that class)\n",
    "\n",
    "Bonus: Logarithms are useful in this situation because they \"undo\" exponential equations (because they are the inverse). So because we got $\\hat{p}$ using normalizing exponentials, taking the natural log helps us to correlate gradients more closely with larger mistakes in prediction.\n",
    "\n",
    "\n",
    "$${J}(\\mathbf{\\Theta}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}y_k^{(i)} log(\\hat{p}_k^i)$$\n",
    "\n",
    "Here's how we can implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:02.064829Z",
     "iopub.status.busy": "2021-12-06T00:13:02.064068Z",
     "iopub.status.idle": "2021-12-06T00:13:02.066902Z",
     "shell.execute_reply": "2021-12-06T00:13:02.066446Z",
     "shell.execute_reply.started": "2021-12-05T23:54:39.58747Z"
    },
    "papermill": {
     "duration": 0.029793,
     "end_time": "2021-12-06T00:13:02.066992",
     "exception": false,
     "start_time": "2021-12-06T00:13:02.037199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xentropyloss(y, p_hat):\n",
    "    epsilon = 1e-6\n",
    "    #we will add epsilon (really small number) to p_hat before taking the log to avoid p_hat being 0\n",
    "    loss = -np.mean(np.sum(np.log(p_hat + epsilon) * y,axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021942,
     "end_time": "2021-12-06T00:13:02.110910",
     "exception": false,
     "start_time": "2021-12-06T00:13:02.088968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross Entropy Gradient Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023264,
     "end_time": "2021-12-06T00:13:02.156483",
     "exception": false,
     "start_time": "2021-12-06T00:13:02.133219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is the derivative of the cost function that we just defined above. This will allow us to perform gradient descent in order to optimize the cost function (lower the amount of error). \n",
    "\n",
    "$\\mathbf{\\nabla}$ (nabla) Is the math symbol for a gradient.\n",
    "\n",
    "$\\sum_{i=1}^{m}$ Takes the sum of the difference between the ground truth and the probability for all instances in the batch.\n",
    "\n",
    "Multiplying by $\\frac{1}{m}$ will give us the average cost over the entire batch.\n",
    "\n",
    "By multiplying by $\\mathbf{x}$ you effect each pixel represented in the handwritten digit.\n",
    "\n",
    "$$\\mathbf{\\nabla}_{\\theta_{k}} J(\\mathbf{\\Theta}) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-{y}_k^{(i)})\\mathbf{x}^{(i)} $$\n",
    "\n",
    "Here's how we can implement it (again, broken up so as to not be too computationally expensive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:02.206935Z",
     "iopub.status.busy": "2021-12-06T00:13:02.206221Z",
     "iopub.status.idle": "2021-12-06T00:13:02.209205Z",
     "shell.execute_reply": "2021-12-06T00:13:02.208704Z",
     "shell.execute_reply.started": "2021-12-05T23:54:40.142754Z"
    },
    "papermill": {
     "duration": 0.03024,
     "end_time": "2021-12-06T00:13:02.209293",
     "exception": false,
     "start_time": "2021-12-06T00:13:02.179053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GradientStep(p_hat, theta, y, x, lr, m):\n",
    "    cost = p_hat - y\n",
    "    gradients = 1/m * x.T.dot(cost)\n",
    "    theta = theta - lr * gradients\n",
    "    return theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:13:02.327234Z",
     "iopub.status.busy": "2021-12-06T00:13:02.262795Z",
     "iopub.status.idle": "2021-12-06T00:15:16.623037Z",
     "shell.execute_reply": "2021-12-06T00:15:16.623605Z",
     "shell.execute_reply.started": "2021-12-05T23:56:30.150824Z"
    },
    "papermill": {
     "duration": 134.391621,
     "end_time": "2021-12-06T00:15:16.623812",
     "exception": false,
     "start_time": "2021-12-06T00:13:02.232191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 10.878938871226115\n",
      "500 : 0.6439486972988038\n",
      "1000 : 0.4799218097438682\n",
      "1500 : 0.4025913981204155\n",
      "2000 : 0.3550522976946002\n",
      "2500 : 0.3221165237248758\n",
      "3000 : 0.2974831779672073\n",
      "3500 : 0.2780477168166539\n",
      "4000 : 0.26214999479750045\n",
      "4500 : 0.2488207021453448\n",
      "5000 : 0.2374423067624096\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "n_inputs = X_train.shape[1]\n",
    "m = 10000 #batch size\n",
    "n_outputs = len(np.unique(y))\n",
    "epochs = 5001\n",
    "np.random.seed(2042)\n",
    "theta = np.random.randn(n_inputs, n_outputs)\n",
    "for epoch in range(epochs):\n",
    "    score = SoftmaxScore(X_train[:m], theta)\n",
    "    p_hat = SoftmaxFunction(score)\n",
    "    if epoch % 500 == 0:\n",
    "        loss = xentropyloss(y_train_one_hot[:m], p_hat)\n",
    "        print(epoch, ':', loss )\n",
    "    theta = GradientStep(p_hat, theta, y_train_one_hot[:m], X_train[:m], lr, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:15:16.681208Z",
     "iopub.status.busy": "2021-12-06T00:15:16.680095Z",
     "iopub.status.idle": "2021-12-06T00:15:16.823657Z",
     "shell.execute_reply": "2021-12-06T00:15:16.823110Z",
     "shell.execute_reply.started": "2021-12-05T23:58:53.203054Z"
    },
    "papermill": {
     "duration": 0.17385,
     "end_time": "2021-12-06T00:15:16.823779",
     "exception": false,
     "start_time": "2021-12-06T00:15:16.649929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8795714285714286"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = SoftmaxScore(X_val, theta)\n",
    "p_hat = SoftmaxFunction(score)\n",
    "pred = Prediction(p_hat)\n",
    "\n",
    "accuracy_score = np.mean(pred == categorical_to_num(y_val))\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026614,
     "end_time": "2021-12-06T00:15:16.875960",
     "exception": false,
     "start_time": "2021-12-06T00:15:16.849346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the sake of comparing to a baseline. Notice that the score that is achievable with the pre-built Scikit-Learn softmax regressor is very close to our accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-06T00:15:16.933955Z",
     "iopub.status.busy": "2021-12-06T00:15:16.933315Z",
     "iopub.status.idle": "2021-12-06T00:15:41.189634Z",
     "shell.execute_reply": "2021-12-06T00:15:41.189165Z",
     "shell.execute_reply.started": "2021-12-05T23:40:45.297186Z"
    },
    "papermill": {
     "duration": 24.287982,
     "end_time": "2021-12-06T00:15:41.189755",
     "exception": false,
     "start_time": "2021-12-06T00:15:16.901773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814285714285715"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10, max_iter = 5001)\n",
    "softmax_reg.fit(X_train[:10000], y_train[:10000])\n",
    "val_predictions = softmax_reg.predict(X_val)\n",
    "val_score = accuracy_score(y_val, val_predictions)\n",
    "val_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 215.981102,
   "end_time": "2021-12-06T00:15:41.326537",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-06T00:12:05.345435",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
